{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeQtBt5JpKpq",
        "colab_type": "text"
      },
      "source": [
        "Đọc dữ liệu từ files và xây dụng từ điển"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A16G4M_as97u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import re\n",
        "import os\n",
        "def gen_data_and_vocab():\n",
        "  def collect_data_from(parent_path, newsgroup_list, word_count = 'None'):\n",
        "    data = []\n",
        "    for group_id, newsgroup in enumerate(newsgroup_list):\n",
        "      dir_path = parent_path + '\\\\' + newsgroup +'\\\\'\n",
        "      files = [(filename, dir_path + filename) for filename in os.listdir(dir_path)]  \n",
        "      files.sort()\n",
        "      label = group_id\n",
        "      print(\"processing: {}-{}\".format(group_id, newsgroup))\n",
        "      for filename, filepath in files:\n",
        "        with open(filepath) as f:\n",
        "          text = f.read().lower()\n",
        "          words = re.split('\\W+', text)\n",
        "          if word_count == 'None':\n",
        "            for word in words:\n",
        "              word_count[word] += 1\n",
        "          content = ' '.join(words)\n",
        "          assert len(content.splitlines()) == 1\n",
        "          data.append(str(label) + '<fff>' + filename + '<fff>' + content)\n",
        "\n",
        "    return data\n",
        "\n",
        "  word_count = defaultdict(int)\n",
        "  path = \"C:\\\\Users\\\\pl\\\\Downloads\\\\20news-bydate\"\n",
        "  parts = [path +\"\\\\\"+ dir_name for dir_name in os.listdir(path)]\n",
        "  train_path, test_path = (parts[0], parts[1]) if \"train\" in parts[0] else (parts[1], parts[0])\n",
        "  newsgroup_list = [newsgroup for newsgroup in os.listdir(train_path)]\n",
        "  newsgroup_list.sort()\n",
        "  \n",
        "  train_data = collect_data_from(\n",
        "      parent_path = train_path,\n",
        "      newsgroup_list = newsgroup_list,\n",
        "      word_count = word_count\n",
        "    )\n",
        "  vocab = [word for word, freq in word_count.items() if freq > 10]\n",
        "  vocab.sort()\n",
        "  with open(\"C:\\\\Users\\\\pl\\\\Downloads\\\\20news-bydate\\\\vocab-raw.txt\",'w') as f:\n",
        "    f.write('\\n'.join(vocab))\n",
        "  test_data = collect_data_from(\n",
        "      parent_path = test_path,\n",
        "      newsgroup_list = newsgroup_list\n",
        "    )\n",
        "  with open(\"C:\\\\Users\\\\pl\\\\Downloads\\\\20news-bydate\\\\20news-train-raw.txt\",'w') as f:\n",
        "    f.write('\\n'.join(train_data))\n",
        "  with open(\"C:\\\\Users\\\\pl\\\\Downloads\\\\20news-bydate\\\\20news-test-raw.txt\",'w') as f:\n",
        "    f.write('\\n'.join(test_data))\n",
        "\n",
        "gen_data_and_vocab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0m15KuTrKhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_DOC_LENGTH = 500\n",
        "unknown_ID = 0\n",
        "padding_ID = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ruf5vA1wpvcb",
        "colab_type": "text"
      },
      "source": [
        "Encode dữ liệu\n",
        "\n",
        "Các từ trong từ điển được đánh ID = 2,3,4,...,V+2\n",
        "\n",
        "Các từ không xuất hiện trong từ điển có ID = 0\n",
        "\n",
        "Các từ rỗng được thêm vào có ID = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vp1DfUeUJqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_DOC_LENGTH = 500\n",
        "unknown_ID = 0\n",
        "padding_ID = 1\n",
        "\n",
        "def encode_data(data_path, vocab_path):\n",
        "  with open(vocab_path,encoding='latin-1') as f:\n",
        "    vocab = dict([(word, word_ID + 2) \n",
        "                  for word_ID, word in enumerate(f.read().splitlines())])\n",
        "  with open(data_path,encoding='latin-1') as f :\n",
        "    documents = f.read().splitlines()\n",
        "  encoded_data = []\n",
        "  for document in documents:\n",
        "    label, doc_id, text = document.split('<fff>')\n",
        "    words = text.split()[:MAX_DOC_LENGTH]\n",
        "    sentence_length = len(words)\n",
        "    encoded_text = []\n",
        "    for word in words:\n",
        "      if word in vocab:\n",
        "        encoded_text.append(str(vocab[word]))\n",
        "      else:\n",
        "        encoded_text.append(str(unknown_ID))\n",
        "    if len(words) < MAX_DOC_LENGTH:\n",
        "      num_padding = MAX_DOC_LENGTH - len(words)\n",
        "      for i in range(num_padding):\n",
        "        encoded_text.append(str(padding_ID))\n",
        "    encoded_data.append(str(label) + '<fff>' + str(doc_id) + '<fff>' + \n",
        "                        str(sentence_length) + '<fff>' + ' '.join(encoded_text))\n",
        "\n",
        "  dir_name = '/'.join(data_path.split('/')[:-1])\n",
        "  file_name = '-'.join(data_path.split('/')[-1].split('-')[:-1]) + '-encoded.txt'\n",
        "  with open(dir_name + '/' +file_name, 'w') as f:\n",
        "    f.write('\\n'.join(encoded_data))\n",
        "\n",
        "encode_data(data_path='/content/drive/My Drive/Data_Colab/20news-train-raw.txt',\n",
        "            vocab_path='/content/drive/My Drive/Data_Colab/vocab-raw.txt')\n",
        "encode_data(data_path='/content/drive/My Drive/Data_Colab/20news-test-raw.txt',\n",
        "            vocab_path='/content/drive/My Drive/Data_Colab/vocab-raw.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiN5l-jzfB7I",
        "colab_type": "text"
      },
      "source": [
        "Get data reader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8lH6vQ5fBN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class DataReader():\n",
        "  def __init__(self, data, labels, sentence_lengths):\n",
        "    self._data = data\n",
        "    self._labels = labels\n",
        "    self._sentence_lengths = sentence_lengths\n",
        "    self._num_epoch = 0\n",
        "    self._batch_id = 0\n",
        "\n",
        "  def reset(self):\n",
        "    self._num_epoch = 0\n",
        "    self._batch_id = 0\n",
        "\n",
        "  def next_batch(self, batch_size):\n",
        "    start = self._batch_id * batch_size\n",
        "    end = start + batch_size\n",
        "    self._batch_id += 1\n",
        "    if end + batch_size > len(self._data):\n",
        "      self._num_epoch += 1\n",
        "      self._batch_id = 0\n",
        "      indices = list(range(len(self._data)))\n",
        "      np.random.seed(2020)\n",
        "      np.random.shuffle(indices)\n",
        "      self._data, self._labels, self._sentence_lengths = self._data[indices], self._labels[indices], self._sentence_lengths[indices]\n",
        "    return self._data[start:end], self._labels[start:end], self._sentence_lengths[start:end]\n",
        "\n",
        "def load_data(data_path):\n",
        "  with open(data_path, encoding = 'latin1') as f:\n",
        "    d_lines = f.read().splitlines()\n",
        "  data, labels, sentence_lengths = [], [], []\n",
        "  for line in d_lines:\n",
        "    features = line.split('<fff>')\n",
        "    label, doc_id, sentence_len = int(features[0]), int(features[1]), int(features[2])\n",
        "    vector = [int(ID) for ID in features[3].split()]\n",
        "    data.append(vector)\n",
        "    labels.append(label)\n",
        "    sentence_lengths.append(sentence_len)\n",
        "  return np.array(data), np.array(labels), np.array(sentence_lengths)\n",
        "  \n",
        "train_data, train_labels, train_sentence_lengths = load_data(\n",
        "    data_path='/content/drive/My Drive/Data_Colab/20news-train-encoded.txt'\n",
        ")\n",
        "test_data, test_labels, test_sentence_lengths = load_data(\n",
        "    data_path='/content/drive/My Drive/Data_Colab/20news-test-encoded.txt'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBIC7wQEqrmj",
        "colab_type": "text"
      },
      "source": [
        "Xây dựng mô hình"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zLiiuPJlnJ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "bdd810b6-3073-4404-f8d2-21527589b8f9"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "import numpy as np\n",
        "\n",
        "MAX_DOC_LENGTH = 500\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "class RNN:\n",
        "  def __init__(self, vocab_size, embedding_size, lstm_size, batch_size):\n",
        "    self._vocab_size = vocab_size\n",
        "    self._embedding_size = embedding_size\n",
        "    self._lstm_size = lstm_size\n",
        "    self._batch_size = batch_size\n",
        "\n",
        "    self._data = tf.placeholder(tf.int32, shape=[batch_size, MAX_DOC_LENGTH])\n",
        "    self._labels = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
        "    self._sentence_lengths = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
        "    self._final_tokens = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
        "\n",
        "  def embedding_layer(self, indices):\n",
        "    pretrained_vectors = []\n",
        "    pretrained_vectors.append(np.zeros(self._embedding_size))\n",
        "    for _ in range(self._vocab_size + 1):\n",
        "      pretrained_vectors.append(np.random.normal(loc=0, scale=1, size=self._embedding_size))\n",
        "\n",
        "    pretrained_vectors = np.array(pretrained_vectors)\n",
        "    self._embedding_matrix = tf.get_variable(\n",
        "        name='embedding',\n",
        "        shape=(self._vocab_size + 2,self._embedding_size),\n",
        "        initializer=tf.constant_initializer(pretrained_vectors)\n",
        "    )\n",
        "    return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n",
        "\n",
        "  def LSTM_layer(self, embeddings):\n",
        "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n",
        "    zero_state = tf.zeros(shape=(self._batch_size, self._lstm_size))\n",
        "    initial_state = tf.nn.rnn_cell.LSTMStateTuple(zero_state, zero_state)\n",
        "\n",
        "    lstm_inputs = tf.unstack(\n",
        "        tf.transpose(embeddings, perm=[1,0,2])\n",
        "    )\n",
        "    lstm_outputs, last_state = tf.nn.static_rnn(\n",
        "        cell=lstm_cell,\n",
        "        inputs=lstm_inputs,\n",
        "        initial_state=initial_state,\n",
        "        sequence_length=self._sentence_lengths\n",
        "    )# [num_docs, lstm_size]\n",
        "    lstm_outputs = tf.unstack(\n",
        "        tf.transpose(lstm_outputs, perm=[1,0,2])\n",
        "    )\n",
        "    lstm_outputs = tf.concat(\n",
        "        lstm_outputs, axis=0\n",
        "    )# [num_docs*MAX_SENT_LENGTH, lstm_size]\n",
        "    mask = tf.sequence_mask(\n",
        "        lengths=self._sentence_lengths,\n",
        "        maxlen=MAX_DOC_LENGTH,\n",
        "        dtype=tf.float32\n",
        "    )# [num_docs, MAX_SENTENCE_LENGTH]\n",
        "    mask = tf.concat(tf.unstack(mask, axis=0), axis=0)\n",
        "    mask = tf.expand_dims(mask, -1)\n",
        "    lstm_outputs = mask * lstm_outputs\n",
        "    lstm_outputs_split = tf.split(lstm_outputs, \n",
        "                                  num_or_size_splits=self._batch_size)\n",
        "    lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split, axis=1)#[num_docs, lstm_size]\n",
        "    lstm_outputs_average = lstm_outputs_sum / tf.expand_dims(\n",
        "        tf.cast(self._sentence_lengths, tf.float32), -1)#[num_docs, lstm_size]\n",
        "    return lstm_outputs_average\n",
        "\n",
        "  def build_graph(self):\n",
        "    embeddings = self.embedding_layer(self._data)\n",
        "    lstm_outputs = self.LSTM_layer(embeddings)\n",
        "\n",
        "    weights = tf.get_variable(\n",
        "        name='final_layer_weights',\n",
        "        shape=(self._lstm_size, NUM_CLASSES),\n",
        "        initializer=tf.random_normal_initializer(seed=2020)\n",
        "    )\n",
        "    biases = tf.get_variable(\n",
        "        name='final_layer_biases',\n",
        "        shape=(NUM_CLASSES),\n",
        "        initializer=tf.random_normal_initializer(seed=2020)\n",
        "    )\n",
        "    logits = tf.matmul(lstm_outputs, weights) + biases\n",
        "    labels_one_hot = tf.one_hot(\n",
        "        indices=self._labels,\n",
        "        depth=NUM_CLASSES,\n",
        "        dtype=tf.float32\n",
        "    )\n",
        "\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
        "        labels=labels_one_hot,\n",
        "        logits=logits\n",
        "    )\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    probs = tf.nn.softmax(logits)\n",
        "    predicted_labels = tf.argmax(probs, axis=1)\n",
        "    predicted_labels = tf.squeeze(predicted_labels)\n",
        "    return predicted_labels, loss\n",
        "\n",
        "  def trainer(self, loss, learning_rate):\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "    return train_op\n",
        "\n",
        "#train and compute accuracy\n",
        "def train_and_evaluate_RNN(lstm_size, batch_size):\n",
        "  acc=[]\n",
        "  with open('/content/drive/My Drive/Data_Colab/vocab-raw.txt',encoding='latin-1') as f:\n",
        "    vocab_size = len(f.read().splitlines())\n",
        "  train_data_reader = DataReader(\n",
        "    data=train_data, \n",
        "    labels=train_labels, \n",
        "    sentence_lengths=train_sentence_lengths\n",
        "  )\n",
        "  test_data_reader = DataReader(\n",
        "    data=test_data, \n",
        "    labels=test_labels, \n",
        "    sentence_lengths=test_sentence_lengths\n",
        "  )\n",
        "  tf.reset_default_graph()\n",
        "  tf.set_random_seed(2020)\n",
        "  rnn = RNN(\n",
        "      vocab_size=vocab_size,\n",
        "      embedding_size=300,\n",
        "      lstm_size=lstm_size,\n",
        "      batch_size=batch_size\n",
        "  )\n",
        "  predicted_labels, loss = rnn.build_graph()\n",
        "  train_op = rnn.trainer(loss=loss, learning_rate=0.01)\n",
        "  \n",
        "  with tf.Session() as sess:\n",
        "    step = 0\n",
        "    MAX_STEP = 1e6\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    while step < MAX_STEP:\n",
        "      next_train_batch = train_data_reader.next_batch(batch_size)\n",
        "      data, labels, sentence_lengths = next_train_batch\n",
        "      plabels_eval, loss_eval, _ = sess.run(\n",
        "          [predicted_labels, loss, train_op],\n",
        "          feed_dict={\n",
        "              rnn._data: data,\n",
        "              rnn._labels: labels,\n",
        "              rnn._sentence_lengths: sentence_lengths,\n",
        "          }\n",
        "      )\n",
        "      step += 1\n",
        "      # if step % 20 == 0:\n",
        "      #   print('loss:', loss_eval)\n",
        "      if train_data_reader._batch_id == 0:\n",
        "        num_true_preds = 0\n",
        "        while True:\n",
        "          next_test_batch = test_data_reader.next_batch(batch_size)\n",
        "          data, labels, sentence_lengths = next_test_batch\n",
        "          test_plabels_eval = sess.run(\n",
        "              predicted_labels,\n",
        "              feed_dict={\n",
        "                  rnn._data: data,\n",
        "                  rnn._labels: labels,\n",
        "                  rnn._sentence_lengths: sentence_lengths,\n",
        "              }\n",
        "          )\n",
        "          matches = np.equal(test_plabels_eval, labels)\n",
        "          num_true_preds += np.sum(matches.astype(float))\n",
        "\n",
        "          if test_data_reader._batch_id == 0:\n",
        "            break\n",
        "        print('Epoch:', train_data_reader._num_epoch)\n",
        "        print('Accuracy on test data:', num_true_preds*100./len(test_data_reader._data))\n",
        "        acc.append(num_true_preds*100./len(test_data_reader._data))\n",
        "  return acc\n",
        "\n",
        "acc = train_and_evaluate_RNN(\n",
        "     lstm_size=80, \n",
        "     batch_size=10)\n",
        "\n",
        "acc = [str(i) for i in acc]\n",
        "with open('/content/drive/My Drive/Data_Colab/RNN_acc.txt') as f:\n",
        "  f.write('\\n'.join(acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From <ipython-input-3-3178f74f9055>:35: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-3-3178f74f9055>:46: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:740: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:744: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-3-3178f74f9055>:92: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Epoch: 1\n",
            "Accuracy on test data: 3.186404673393521\n",
            "Epoch: 2\n",
            "Accuracy on test data: 75.7700477960701\n",
            "Epoch: 3\n",
            "Accuracy on test data: 76.9383961763144\n",
            "Epoch: 4\n",
            "Accuracy on test data: 76.72596919808815\n",
            "Epoch: 5\n",
            "Accuracy on test data: 77.72172065852364\n",
            "Epoch: 6\n",
            "Accuracy on test data: 77.33669676048858\n",
            "Epoch: 7\n",
            "Accuracy on test data: 76.81890600106213\n",
            "Epoch: 8\n",
            "Accuracy on test data: 74.09718534253851\n",
            "Epoch: 9\n",
            "Accuracy on test data: 75.02655337227829\n",
            "Epoch: 10\n",
            "Accuracy on test data: 75.87626128518322\n",
            "Epoch: 11\n",
            "Accuracy on test data: 76.31439192777482\n",
            "Epoch: 12\n",
            "Accuracy on test data: 76.76579925650557\n",
            "Epoch: 13\n",
            "Accuracy on test data: 76.2081784386617\n",
            "Epoch: 14\n",
            "Accuracy on test data: 74.73446627721721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsHrh89rc924",
        "colab_type": "text"
      },
      "source": [
        "Cross validation tim best lstm size and batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0jTbRaqj4OZ",
        "colab_type": "code",
        "outputId": "3ff62462-142e-42d8-a77e-739cf234637a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "with open('/content/drive/My Drive/Data_Colab/vocab-raw.txt',encoding='latin-1') as f:\n",
        "  vocab_size = len(f.read().splitlines())\n",
        "\n",
        "def get_the_best_parameter(data, labels, sentence_lengths):\n",
        "  \n",
        "  def compute_RSS(Y_new,Y_predicted):\n",
        "    return (1/Y_new.shape[0])*np.sum((Y_new-Y_predicted)**2)\n",
        "\n",
        "  def cross_validation(num_folds, lstm_size, batch_size):\n",
        "    row_ids=np.array(range(data.shape[0]))\n",
        "    valid_ids=np.split(row_ids[:len(row_ids)-len(row_ids)%num_folds],num_folds)\n",
        "    valid_ids[-1]=np.append(valid_ids[-1],row_ids[len(row_ids)-len(row_ids)%num_folds:])\n",
        "    train_ids=[[k for k in row_ids if k not in valid_ids[i]] for i in range(num_folds)]\n",
        "    aver_RSS = 0\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(2020)\n",
        "    rnn = RNN(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_size=300,\n",
        "        lstm_size=lstm_size,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    predicted_labels, loss = rnn.build_graph()\n",
        "    train_op = rnn.trainer(loss=loss, learning_rate=1)\n",
        "    for i in range(num_folds):\n",
        "      valid_part={'data':data[valid_ids[i]], 'labels':labels[valid_ids[i]], 'sentence_lengths':sentence_lengths[valid_ids[i]]}\n",
        "      train_part={'data':data[train_ids[i]], 'labels':labels[train_ids[i]], 'sentence_lengths':sentence_lengths[train_ids[i]]}\n",
        "      train_data_reader = DataReader(\n",
        "          data=train_part['data'], \n",
        "          labels=train_part['labels'], \n",
        "          sentence_lengths=train_part['sentence_lengths']\n",
        "      )\n",
        "      valid_data_reader = DataReader(\n",
        "          data=valid_part['data'], \n",
        "          labels=valid_part['labels'], \n",
        "          sentence_lengths=valid_part['sentence_lengths']\n",
        "      )\n",
        "      with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        while True:\n",
        "          batch_data, batch_labels, batch_sentence_lengths = train_data_reader.next_batch(batch_size)\n",
        "          plabels_eval, loss_eval, _ = sess.run(\n",
        "              [predicted_labels, loss, train_op],\n",
        "              feed_dict={\n",
        "                  rnn._data: batch_data,\n",
        "                  rnn._labels: batch_labels,\n",
        "                  rnn._sentence_lengths: batch_sentence_lengths,\n",
        "              }\n",
        "          )\n",
        "          if train_data_reader._batch_id == 0 and train_data_reader._num_epoch == 2:\n",
        "            break\n",
        "\n",
        "        RSS = 0\n",
        "        while True:\n",
        "          batch_data, batch_labels, batch_sentence_lengths = valid_data_reader.next_batch(batch_size)\n",
        "          valid_plabels_eval = sess.run(\n",
        "              predicted_labels,\n",
        "              feed_dict={\n",
        "                  rnn._data: batch_data,\n",
        "                  rnn._labels: batch_labels,\n",
        "                  rnn._sentence_lengths: batch_sentence_lengths,\n",
        "              }\n",
        "          )\n",
        "          RSS += compute_RSS(batch_labels, valid_plabels_eval)\n",
        "          if valid_data_reader._batch_id == 0:\n",
        "            break\n",
        "      print('RSS:',RSS)\n",
        "      aver_RSS += RSS\n",
        "    print('aver RSS:', aver_RSS)\n",
        "    return aver_RSS/num_folds\n",
        "  \n",
        "  def range_scan(lstm_size_values, batch_size_values):\n",
        "    best_lstm_size = 50\n",
        "    min_RSS = 1e8\n",
        "    for current_lstm_size in lstm_size_values:\n",
        "      aver_RSS = cross_validation(\n",
        "          num_folds=5, \n",
        "          lstm_size=current_lstm_size, \n",
        "          batch_size=50\n",
        "      )\n",
        "      if aver_RSS<min_RSS:\n",
        "        best_lstm_size = current_lstm_size\n",
        "        min_RSS=aver_RSS\n",
        "\n",
        "    best_batch_size = 50\n",
        "    min_RSS = 1e8\n",
        "    for current_batch_size in batch_size_values:\n",
        "      aver_RSS = cross_validation(\n",
        "          num_folds=5, \n",
        "          lstm_size=50, \n",
        "          batch_size=current_batch_size\n",
        "      )\n",
        "      if aver_RSS<min_RSS:\n",
        "        best_batch_size = current_batch_size\n",
        "        min_RSS=aver_RSS\n",
        "        return best_lstm_size, best_batch_size\n",
        "\n",
        "  lstm_size_values = [i*10 for i in range(1,10)]\n",
        "  batch_size_values = lstm_size_values\n",
        "  best_lstm_size, best_batch_size = range_scan(lstm_size_values, batch_size_values)\n",
        "  return best_lstm_size, best_batch_size\n",
        "\n",
        "best_lstm_size, best_batch_size = get_the_best_parameter(train_data, train_labels, train_sentence_lengths)\n",
        "print(\"best lstm size:\", best_lstm_size)\n",
        "print(\"best batch size:\", best_batch_size)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From <ipython-input-2-481d1d349770>:35: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-2-481d1d349770>:46: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:740: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:744: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-2-481d1d349770>:92: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "RSS: 5189.279999999999\n",
            "RSS: 2491.3999999999996\n",
            "RSS: 1685.68\n",
            "RSS: 2477.38\n",
            "RSS: 5943.620000000001\n",
            "aver RSS: 17787.36\n",
            "RSS: 5139.280000000001\n",
            "RSS: 2347.4\n",
            "RSS: 2172.8\n",
            "RSS: 2511.320000000001\n",
            "RSS: 4460.38\n",
            "aver RSS: 16631.18\n",
            "RSS: 5846.960000000002\n",
            "RSS: 3090.960000000001\n",
            "RSS: 1868.9999999999995\n",
            "RSS: 3144.68\n",
            "RSS: 7169.499999999999\n",
            "aver RSS: 21121.100000000002\n",
            "RSS: 6038.099999999999\n",
            "RSS: 2587.480000000001\n",
            "RSS: 2258.74\n",
            "RSS: 3056.379999999999\n",
            "RSS: 5396.9000000000015\n",
            "aver RSS: 19337.6\n",
            "RSS: 5019.48\n",
            "RSS: 2159.92\n",
            "RSS: 1741.1400000000003\n",
            "RSS: 2959.7799999999997\n",
            "RSS: 5889.940000000002\n",
            "aver RSS: 17770.260000000002\n",
            "RSS: 6035.780000000001\n",
            "RSS: 2588.26\n",
            "RSS: 1846.22\n",
            "RSS: 3495.62\n",
            "RSS: 4605.879999999999\n",
            "aver RSS: 18571.760000000002\n",
            "RSS: 8019.500000000001\n",
            "RSS: 2292.12\n",
            "RSS: 1807.9400000000003\n",
            "RSS: 2967.6\n",
            "RSS: 5783.799999999998\n",
            "aver RSS: 20870.96\n",
            "RSS: 4442.100000000001\n",
            "RSS: 2129.6\n",
            "RSS: 1664.5399999999997\n",
            "RSS: 2651.5200000000004\n",
            "RSS: 4309.919999999998\n",
            "aver RSS: 15197.679999999998\n",
            "RSS: 3534.7200000000007\n",
            "RSS: 2396.6199999999994\n",
            "RSS: 1821.7200000000003\n",
            "RSS: 2765.0\n",
            "RSS: 6703.159999999998\n",
            "aver RSS: 17221.22\n",
            "RSS: 15977.200000000015\n",
            "RSS: 8721.00000000001\n",
            "RSS: 8344.9\n",
            "RSS: 14841.000000000011\n",
            "RSS: 23116.100000000006\n",
            "aver RSS: 71000.20000000004\n",
            "best lstm size: 80\n",
            "best batch size: 10\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}