{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "session-3-MLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpdOQErxXzmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1BG_X82zL9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "import numpy as np\n",
        "\n",
        "class MLP:\n",
        "  def __init__(self, vocab_size, hidden_size, num_classes):\n",
        "    self._vocab_size = vocab_size\n",
        "    self._hidden_size = hidden_size\n",
        "    self._num_classes = num_classes\n",
        "  def build_graph(self):\n",
        "    self._X = tf.placeholder(tf.float32, shape = [None, self._vocab_size])\n",
        "    self._real_Y = tf.placeholder(tf.int32, shape=[None, ])\n",
        "    \n",
        "    weights_1 = tf.get_variable(\n",
        "        name = 'hidden_input_weights',\n",
        "        shape = (self._vocab_size, self._hidden_size),\n",
        "        initializer = tf.random_normal_initializer(seed = 2020)\n",
        "    )\n",
        "    biases_1=tf.get_variable(\n",
        "        name = 'hidden_input_biases',\n",
        "        shape = (self._hidden_size),\n",
        "        initializer = tf.random_normal_initializer(seed = 2020)\n",
        "    )\n",
        "    weights_2 = tf.get_variable(\n",
        "        name = 'hidden_output_weights',\n",
        "        shape = (self._hidden_size, self._num_classes),\n",
        "        initializer = tf.random_normal_initializer(seed = 2020)\n",
        "    )\n",
        "    biases_2 = tf.get_variable(\n",
        "        name = 'hidden_output_biases',\n",
        "        shape = (self._num_classes),\n",
        "        initializer = tf.random_normal_initializer(seed = 2020)\n",
        "    )\n",
        "    hiden = tf.sigmoid(tf.matmul(self._X, weights_1) + biases_1)\n",
        "    logits = tf.matmul(hiden, weights_2) + biases_2\n",
        "    labels_one_hot = tf.one_hot(indices = self._real_Y, depth = self._num_classes, dtype = tf.float32)\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(labels = labels_one_hot, logits = logits)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    probs = tf.nn.softmax(logits)\n",
        "    predicted_labels = tf.argmax(probs, axis=1)\n",
        "    predicted_labels = tf.squeeze(predicted_labels)\n",
        "    return predicted_labels, loss\n",
        "  \n",
        "  def trainer(self, loss, learning_rate):\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "    return train_op\n",
        "\n",
        "class DataReader():\n",
        "  def __init__(self, data_path, batch_size, vocab_size):\n",
        "    self._batch_size = batch_size\n",
        "    with open(data_path, encoding = 'latin1') as f:\n",
        "      d_lines = f.read().splitlines()\n",
        "\n",
        "    self._data, self._labels = [], []\n",
        "    for data_id, line in enumerate(d_lines):\n",
        "      vector = [0.0 for _ in range(vocab_size)]\n",
        "      features = line.split('<fff>')\n",
        "      label,doc_id = int(features[0]), int(features[1])\n",
        "      for token in features[2].split():\n",
        "        index, value = int(token.split(':')[0]), float(token.split(':')[1])\n",
        "        vector[index] = value\n",
        "      self._data.append(vector)\n",
        "      self._labels.append(label)\n",
        "    self._data = np.array(self._data)\n",
        "    self._labels = np.array(self._labels)\n",
        "    self._num_epoch = 0\n",
        "    self._batch_id = 0\n",
        "\n",
        "  def reset(self):\n",
        "    self._num_epoch = 0\n",
        "    self._batch_id = 0\n",
        "\n",
        "  def next_batch(self):\n",
        "    start = self._batch_id * self._batch_size\n",
        "    end = start + self._batch_size\n",
        "    self._batch_id += 1\n",
        "    if end + self._batch_size > len(self._data):\n",
        "      end = len(self._data)\n",
        "      self._num_epoch += 1\n",
        "      self._batch_id = 0\n",
        "      indices = list(range(len(self._data)))\n",
        "      np.random.seed(2020)\n",
        "      np.random.shuffle(indices)\n",
        "      self._data, self._labels = self._data[indices], self._labels[indices]\n",
        "    return self._data[start:end], self._labels[start:end]\n",
        "\n",
        "def load_dataset(vocab_size):\n",
        "  train_data_reader = DataReader(\n",
        "      data_path = '/content/drive/My Drive/Data_Colab/train_tf_idf_vector.txt',\n",
        "      batch_size = 50,\n",
        "      vocab_size = vocab_size\n",
        "  )\n",
        "  test_data_reader = DataReader(\n",
        "      data_path = '/content/drive/My Drive/Data_Colab/test_tf_idf_vector.txt',\n",
        "      batch_size = 50,\n",
        "      vocab_size = vocab_size\n",
        "  )\n",
        "  return train_data_reader, test_data_reader\n",
        "\n",
        "def save_parameters(name, value, epoch):\n",
        "  file_name = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
        "  if len(value.shape) == 1:#is a list\n",
        "    string_form = ','.join([str(number) for number in value])\n",
        "  else:\n",
        "    string_form = '\\n'.join([','.join([str(number) for number in value[row]])\n",
        "                                          for row in range(value.shape[0])])\n",
        "  with open(\"/content/drive/My Drive/Data_Colab/saved_paras/\"+file_name, \"w\") as f:\n",
        "    f.write(string_form)\n",
        "\n",
        "def restore_parameters(name, epoch):\n",
        "  file_name = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
        "  with open(\"/content/drive/My Drive/Data_Colab/saved_paras/\"+file_name, \"r\") as f:\n",
        "    lines = f.read().splitlines()\n",
        "  if len(lines) == 1: #is a vector\n",
        "    value = [float(number) for number in lines[0].split(\",\")]\n",
        "  else: #is a matrix\n",
        "    value = [[float(number) for number in lines[row].split(\",\")]\n",
        "             for row in range(len(lines))]\n",
        "  return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOZz4vgrE4n9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/Data_Colab/words_idf.txt',encoding = 'latin1') as f:\n",
        "  vocab_size = len(f.read().splitlines())\n",
        "train_data_reader, test_data_reader = load_dataset(vocab_size) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X9yNHfejORw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "235a1138-3da4-4811-dca0-ba8dc97db449"
      },
      "source": [
        "#create a computation graph\n",
        "tf.reset_default_graph()\n",
        "mlp = MLP(vocab_size = vocab_size,\n",
        "        hidden_size = 50,\n",
        "        num_classes = 20)\n",
        "predicted_labels, loss = mlp.build_graph()\n",
        "train_op = mlp.trainer(loss = loss, learning_rate = 0.1)\n",
        "train_data_reader.reset()\n",
        "test_data_reader.reset()\n",
        "#open a session to run\n",
        "with tf.Session() as sess:\n",
        "  step, MAX_STEP = 0,2e3\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  #training\n",
        "  while step < MAX_STEP:\n",
        "    train_data, train_labels = train_data_reader.next_batch()\n",
        "    plabels_eval, loss_eval, _ = sess.run(\n",
        "        [predicted_labels, loss, train_op],\n",
        "        feed_dict = {\n",
        "            mlp._X: train_data,\n",
        "            mlp._real_Y: train_labels\n",
        "        }\n",
        "    )\n",
        "    step += 1\n",
        "    print('step: {}, loss: {}'.format(step, loss_eval))\n",
        "\n",
        "  #save model parameters\n",
        "  trainable_variables = tf.trainable_variables()\n",
        "  for variable in trainable_variables:\n",
        "    save_parameters(\n",
        "        name = variable.name,\n",
        "        value = variable.eval(),\n",
        "        epoch = train_data_reader._num_epoch\n",
        "    )\n",
        "  #Danh gia model tren test data\n",
        "  trainable_variables = tf.trainable_variables()\n",
        "  for variable in trainable_variables:\n",
        "    saved_value = restore_parameters(variable.name, train_data_reader._num_epoch)\n",
        "    assign_op = variable.assign(saved_value)\n",
        "    sess.run(assign_op)\n",
        "  num_true_preds = 0\n",
        "  while True:\n",
        "    test_data,test_labels = test_data_reader.next_batch()\n",
        "    test_plabels_eval = sess.run(\n",
        "        predicted_labels,\n",
        "        feed_dict = {\n",
        "            mlp._X: test_data,\n",
        "            mlp._real_Y: test_labels\n",
        "        }\n",
        "    )\n",
        "    matches = np.equal(test_plabels_eval, test_labels)\n",
        "    num_true_preds += np.sum(matches.astype(float))\n",
        "\n",
        "    if test_data_reader._batch_id == 0:\n",
        "      break\n",
        "\n",
        "  print('Epoch:', train_data_reader._num_epoch)\n",
        "  print('Accuracy on test data:', num_true_preds/len(test_data_reader._data))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 8\n",
            "Accuracy on test data: 0.7935475305363782\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}